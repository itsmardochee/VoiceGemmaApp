import {initLlama} from 'llama.rn';
import {LlamaContext, RNLlamaOAICompatibleMessage, TokenData} from 'llama.rn';
import FS from 'react-native-fs2/src';

class LlamaService {
  private context: LlamaContext | null = null;

  async downloadModel(
    url: string,
    onProgress?: (progress: number) => void,
  ): Promise<string> {
    try {
      const filename = url.split('/').pop() || 'downloaded-model.gguf';
      const downloadPath = `${FS.DocumentDirectoryPath}/${filename}`;

      console.log('üì• LLM: D√©but du t√©l√©chargement:', url);

      const {promise} = FS.downloadFile({
        fromUrl: url,
        toFile: downloadPath,
        progress: status => {
          const progress = status.bytesWritten / status.contentLength;
          console.log('üì• LLM: Progression:', Math.round(progress * 100) + '%');
          onProgress?.(progress);
        },
      });

      await promise;
      console.log('‚úÖ LLM: T√©l√©chargement termin√©:', downloadPath);
      return downloadPath;
    } catch (error) {
      console.error('‚ùå LLM: Erreur de t√©l√©chargement:', error);
      throw error;
    }
  }

  async initialize(customModelPath?: string) {
    try {
      console.log('üöÄ LLM: Initialisation du service...');
      console.log('üîç LLM: Chemin du mod√®le fourni:', customModelPath);

      let modelPath = customModelPath;

      if (!modelPath) {
        console.log(
          '‚ö†Ô∏è LLM: Aucun chemin personnalis√©, recherche du mod√®le par d√©faut...',
        );
        modelPath = await this.prepareDefaultModel();
      }

      console.log("üìÅ LLM: V√©rification de l'existence du mod√®le:", modelPath);

      // V√©rifier que le fichier existe
      const exists = await FS.exists(modelPath);
      if (!exists) {
        console.error('‚ùå LLM: Fichier mod√®le introuvable:', modelPath);
        throw new Error(`Mod√®le non trouv√©: ${modelPath}`);
      }

      console.log(
        '‚úÖ LLM: Fichier mod√®le trouv√©, taille:',
        await this.getFileSize(modelPath),
      );
      console.log('üìÅ LLM: Utilisation du mod√®le:', modelPath);

      // Param√®tres optimis√©s pour Samsung S24 Ultra
      this.context = await initLlama({
        model: modelPath,
        use_mlock: true,
        n_ctx: 1024, // R√©duit pour des r√©ponses plus rapides
        n_gpu_layers: 99, // Utilise au maximum le GPU du S24 Ultra
        n_threads: 8, // Utilise tous les c≈ìurs performance du S24 Ultra
        n_batch: 64, // Traitement par batch plus efficace
      });

      console.log(
        '‚úÖ LLM: Service initialis√© avec succ√®s avec param√®tres optimis√©s',
      );
    } catch (error) {
      console.error("‚ùå LLM: Erreur lors de l'initialisation:", error);
      throw error;
    }
  }

  private async prepareDefaultModel(): Promise<string> {
    const MODEL_FILENAME = 'gemma-3-4b-it-Q8_0.gguf';
    const BUNDLED_MODEL_PATH = `${FS.MainBundlePath}/${MODEL_FILENAME}`;
    const LOCAL_MODEL_PATH = `${FS.DocumentDirectoryPath}/${MODEL_FILENAME}`;

    try {
      // V√©rifier si le mod√®le existe d√©j√† dans le dossier Documents
      const localExists = await FS.exists(LOCAL_MODEL_PATH);
      if (localExists) {
        console.log('‚úÖ LLM: Mod√®le par d√©faut trouv√© dans le stockage local');
        return LOCAL_MODEL_PATH;
      }

      // V√©rifier si le mod√®le existe dans le bundle de l'app
      const bundledExists = await FS.exists(BUNDLED_MODEL_PATH);
      if (bundledExists) {
        console.log('üì¶ LLM: Copie du mod√®le depuis le bundle...');
        await FS.copyFile(BUNDLED_MODEL_PATH, LOCAL_MODEL_PATH);
        console.log('‚úÖ LLM: Mod√®le copi√© avec succ√®s');
        return LOCAL_MODEL_PATH;
      }

      throw new Error(
        'Aucun mod√®le par d√©faut trouv√©. Veuillez s√©lectionner ou t√©l√©charger un mod√®le.',
      );
    } catch (err) {
      console.error(
        '‚ùå LLM: Erreur lors de la pr√©paration du mod√®le par d√©faut:',
        err,
      );
      throw err;
    }
  }

  private async getFileSize(filePath: string): Promise<string> {
    try {
      const stat = await FS.stat(filePath);
      const sizeMB = Math.round(stat.size / (1024 * 1024));
      return `${sizeMB} MB`;
    } catch {
      return 'Taille inconnue';
    }
  }

  async completion(
    messages: RNLlamaOAICompatibleMessage[],
    onPartialCompletion?: (data: TokenData) => void,
  ) {
    try {
      if (!this.context) {
        throw new Error('LLM context not initialized');
      }

      console.log('ü§ñ LLM: G√©n√©ration de la r√©ponse...');
      const res = await this.context.completion(
        {
          messages,
          n_predict: 100, // Encore plus court pour √©viter les r√©p√©titions
          temperature: 0.8,
          top_k: 40,
          top_p: 0.9,
          ignore_eos: false,
          stop: [
            '<end_of_turn>',
            '<|end_of_text|>',
            '<|endoftext|>',
            '</s>',
            '<eos>',
            '\n\n\n',
            'User:',
            'Human:',
            'Assistant:',
          ],
        },
        onPartialCompletion,
      );

      let response = res?.text?.trim() ?? 'Aucune r√©ponse g√©n√©r√©e';

      // Nettoyer la r√©ponse des tokens ind√©sirables
      response = this.cleanResponse(response);

      console.log(
        '‚úÖ LLM: R√©ponse g√©n√©r√©e et nettoy√©e:',
        response.substring(0, 100) + '...',
      );
      return response;
    } catch (err) {
      console.error('‚ùå LLM: Erreur lors de la g√©n√©ration:', err);
      throw err;
    }
  }

  private cleanResponse(text: string): string {
    // Supprimer tous les tokens de fin r√©p√©titifs
    const cleanPatterns = [
      /<end_of_turn>/g,
      /<\|end_of_text\|>/g,
      /<\|endoftext\|>/g,
      /<\/s>/g,
      /<eos>/g,
      /\n\n+/g, // Supprimer les multiples sauts de ligne
    ];

    let cleaned = text;

    // Appliquer tous les patterns de nettoyage
    cleanPatterns.forEach(pattern => {
      cleaned = cleaned.replace(pattern, '');
    });

    // Supprimer les espaces en d√©but et fin
    cleaned = cleaned.trim();

    // Si le texte est vide apr√®s nettoyage, retourner un message par d√©faut
    if (!cleaned || cleaned.length === 0) {
      return "Je n'ai pas pu g√©n√©rer une r√©ponse claire.";
    }

    // Limiter √† la premi√®re phrase compl√®te si la r√©ponse est trop longue
    const sentences = cleaned.split(/[.!?]+/);
    if (sentences.length > 2 && sentences[0].length > 10) {
      return sentences[0].trim() + '.';
    }

    return cleaned;
  }

  async cleanup() {
    try {
      if (this.context) {
        await this.context.release();
        this.context = null;
        console.log('üßπ LLM: Service nettoy√©');
      }
    } catch (error) {
      console.error('‚ùå LLM: Erreur lors du nettoyage:', error);
    }
  }

  isReady(): boolean {
    const ready = this.context !== null;
    console.log("üîç LLM: V√©rification de l'√©tat - Contexte existe:", ready);
    return ready;
  }

  async findExistingModel(fileName: string): Promise<string | null> {
    try {
      const possiblePaths = [
        `${FS.DocumentDirectoryPath}/${fileName}`,
        `${FS.DocumentDirectoryPath}/models/${fileName}`,
        // Ajouter d'autres chemins possibles si n√©cessaire
      ];
      
      for (const path of possiblePaths) {
        const exists = await FS.exists(path);
        if (exists) {
          console.log('‚úÖ LLM: Mod√®le existant trouv√©:', path);
          return path;
        }
      }
      
      // Chercher dans tous les fichiers du r√©pertoire Documents
      const files = await FS.readDir(FS.DocumentDirectoryPath);
      for (const file of files) {
        if (file.name === fileName && file.isFile()) {
          const fullPath = `${FS.DocumentDirectoryPath}/${file.name}`;
          console.log('‚úÖ LLM: Mod√®le existant trouv√© dans le r√©pertoire:', fullPath);
          return fullPath;
        }
      }
      
      console.log('‚ö†Ô∏è LLM: Aucun mod√®le existant trouv√© pour:', fileName);
      return null;
    } catch (error) {
      console.error('‚ùå LLM: Erreur lors de la recherche de mod√®le existant:', error);
      return null;
    }
  }

  async checkFileExists(filePath: string): Promise<boolean> {
    try {
      return await FS.exists(filePath);
    } catch (error) {
      console.error('‚ùå LLM: Erreur lors de la v√©rification du fichier:', error);
      return false;
    }
  }

  async cleanupOldModels(keepPath?: string): Promise<void> {
    try {
      const files = await FS.readDir(FS.DocumentDirectoryPath);
      const modelFiles = files.filter(file => 
        file.isFile() && 
        (file.name.endsWith('.gguf') || file.name.endsWith('.bin')) &&
        (!keepPath || `${FS.DocumentDirectoryPath}/${file.name}` !== keepPath)
      );
      
      for (const file of modelFiles) {
        const fullPath = `${FS.DocumentDirectoryPath}/${file.name}`;
        await FS.unlink(fullPath);
        console.log('üóëÔ∏è LLM: Ancien mod√®le supprim√©:', file.name);
      }
    } catch (error) {
      console.error('‚ùå LLM: Erreur lors du nettoyage:', error);
    }
  }
}

export default new LlamaService();
